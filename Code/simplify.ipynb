{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99cfa4-c77f-4042-9342-28d060ba7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def modify_ply(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Converts all occurrences of scalar_Classification value 5 to 1 (i.e. barrier class to building class)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the .ply file as DataFrame, skipping the header\n",
    "    with open(input_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    header_end = lines.index('end_header\\n') + 1\n",
    "    data = pd.read_csv(input_file, sep=' ', header=None, skiprows=header_end)\n",
    "    \n",
    "    # Modify the scalar_Classification column where it's 5\n",
    "    data[3] = data[3].replace(5.0, 1.0)\n",
    "    \n",
    "    # Write the header and modified data back to a new file\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.writelines(lines[:header_end])  # Write header\n",
    "        data.to_csv(file, header=False, index=False, sep=' ', float_format='%.6f')\n",
    "\n",
    "# Example usage\n",
    "modify_ply('A1-A10_ongoing4.ply', 'A1-A10_ongoing5.ply')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd571ea-8e91-4a7c-9c0d-c7be5b80db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1-A10_ongoing3.ply  A1-A10_ongoing4.ply  Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd11e6-f4b1-4e99-bd66-a5c78dc67e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from: A1-A10_ongoing6.ply\n",
      "Writing to: A1-A10_ongoing7.ply\n",
      "Removing points with scalar_Classification = 12.0...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def process_ply_file(input_filename, output_filename, classification_to_remove=12.0):\n",
    "    \"\"\"\n",
    "    Reads an ASCII PLY file, performs multiple cleaning and reformatting operations,\n",
    "    and writes the result to a new file.\n",
    "\n",
    "    Operations:\n",
    "    1. Remove points with duplicate (x, y, z) coordinates.\n",
    "    2. Remove points where scalar_Classification is equal to the specified value (default 12.0).\n",
    "    3. Remove the 'scalar_Original_cloud_index' property from the header and data.\n",
    "    4. Rename 'scalar_Classification' to 'class' and change its type from float to int.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Reading from: {input_filename}\")\n",
    "    print(f\"Writing to: {output_filename}\")\n",
    "    print(f\"Removing points with scalar_Classification = {classification_to_remove}...\")\n",
    "    \n",
    "    if not os.path.exists(input_filename):\n",
    "        print(f\"Error: Input file not found at '{input_filename}'\")\n",
    "        return\n",
    "\n",
    "    # --- Initializing Counters and Storage ---\n",
    "    header_lines = []\n",
    "    points = []\n",
    "    \n",
    "    # Store unique coordinates to detect duplicates\n",
    "    unique_coords = set()\n",
    "    \n",
    "    # Track the indices of the properties we want to KEEP\n",
    "    x_idx, y_idx, z_idx = -1, -1, -1\n",
    "    classification_idx = -1\n",
    "    \n",
    "    # Final data list after filtering\n",
    "    processed_points = []\n",
    "\n",
    "    # --- 1. Read and Parse the Input File ---\n",
    "    try:\n",
    "        with open(input_filename, 'r') as infile:\n",
    "            in_header = True\n",
    "            \n",
    "            for line in infile:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                if in_header:\n",
    "                    header_lines.append(line)\n",
    "                    if line == 'end_header':\n",
    "                        in_header = False\n",
    "                    continue\n",
    "                \n",
    "                # --- Read Point Data ---\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 5: # Expecting at least x, y, z, classification, index\n",
    "                    points.append(parts)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return\n",
    "        \n",
    "    # --- 2. Process Header to Determine Indices and Create New Header ---\n",
    "    \n",
    "    new_header = []\n",
    "    property_names = []\n",
    "    \n",
    "    for line in header_lines:\n",
    "        new_line = line\n",
    "        \n",
    "        # 2a. Remove 'scalar_Original_cloud_index' and adjust 'scalar_Classification'\n",
    "        if line.startswith('property'):\n",
    "            parts = line.split()\n",
    "            prop_type = parts[1]\n",
    "            prop_name = parts[2]\n",
    "            property_names.append(prop_name)\n",
    "            \n",
    "            if prop_name == 'scalar_Original_cloud_index':\n",
    "                # Skip this property\n",
    "                continue\n",
    "            elif prop_name == 'scalar_Classification':\n",
    "                # Rename and change type\n",
    "                new_line = f\"property int class\"\n",
    "                # Note: Index determination will be done later based on property_names\n",
    "            \n",
    "            elif prop_name in ['x', 'y', 'z']:\n",
    "                # Track original property indices\n",
    "                if prop_name == 'x': x_idx = len(property_names) - 1\n",
    "                if prop_name == 'y': y_idx = len(property_names) - 1\n",
    "                if prop_name == 'z': z_idx = len(property_names) - 1\n",
    "                \n",
    "            new_header.append(new_line)\n",
    "            \n",
    "        elif line.startswith('element vertex'):\n",
    "            # This line will be updated with the final count later\n",
    "            new_header.append(line)\n",
    "        else:\n",
    "            # Keep other header lines (ply, format, comments, end_header)\n",
    "            new_header.append(line)\n",
    "            \n",
    "    # Find the original classification index after tracking all properties\n",
    "    if 'scalar_Classification' in property_names:\n",
    "        classification_idx = property_names.index('scalar_Classification')\n",
    "\n",
    "    if x_idx == -1 or y_idx == -1 or z_idx == -1 or classification_idx == -1:\n",
    "        print(\"Error: Required properties (x, y, z, scalar_Classification) not found in header.\")\n",
    "        return\n",
    "\n",
    "    # --- 3. Filter and Process Points ---\n",
    "    \n",
    "    points_removed_duplicate = 0\n",
    "    points_removed_classification = 0\n",
    "    \n",
    "    for parts in points:\n",
    "        try:\n",
    "            # Get coordinates for duplication check\n",
    "            x = parts[x_idx]\n",
    "            y = parts[y_idx]\n",
    "            z = parts[z_idx]\n",
    "            \n",
    "            # Key for duplicate check (coordinates)\n",
    "            coord_key = (x, y, z)\n",
    "            \n",
    "            # 3a. Check for Duplicates\n",
    "            if coord_key in unique_coords:\n",
    "                points_removed_duplicate += 1\n",
    "                continue\n",
    "            \n",
    "            # 3b. Check for Classification\n",
    "            current_classification = float(parts[classification_idx])\n",
    "            if current_classification == classification_to_remove:\n",
    "                points_removed_classification += 1\n",
    "                continue\n",
    "            \n",
    "            # Add to unique set\n",
    "            unique_coords.add(coord_key)\n",
    "            \n",
    "            # --- 3c. Modify and Format Point ---\n",
    "            new_parts = []\n",
    "            \n",
    "            # Track the original index in the input data\n",
    "            original_index = 0\n",
    "            \n",
    "            for i, val in enumerate(parts):\n",
    "                if i == x_idx or i == y_idx or i == z_idx:\n",
    "                    # Keep x, y, z\n",
    "                    new_parts.append(val)\n",
    "                elif i == classification_idx:\n",
    "                    # Change type from float to int and rename/reformat\n",
    "                    int_class = int(round(current_classification))\n",
    "                    new_parts.append(str(int_class)) # 'class' property, now int\n",
    "                elif property_names[i] == 'scalar_Original_cloud_index':\n",
    "                    # Skip 'scalar_Original_cloud_index' values\n",
    "                    continue\n",
    "                # All other properties are implicitly skipped due to tracking only the new desired properties\n",
    "\n",
    "            processed_points.append(' '.join(new_parts))\n",
    "            \n",
    "        except (ValueError, IndexError) as e:\n",
    "            # Skip malformed lines\n",
    "            print(f\"Skipping malformed data line: {e}\")\n",
    "            \n",
    "    # --- 4. Final Header Assembly and Writing ---\n",
    "    \n",
    "    final_vertex_count = len(processed_points)\n",
    "    \n",
    "    # Replace the 'element vertex' line with the new count\n",
    "    final_header = []\n",
    "    for line in new_header:\n",
    "        if line.startswith('element vertex'):\n",
    "            final_header.append(f\"element vertex {final_vertex_count}\")\n",
    "        else:\n",
    "            final_header.append(line)\n",
    "\n",
    "    try:\n",
    "        with open(output_filename, 'w') as outfile:\n",
    "            # Write Header\n",
    "            for line in final_header:\n",
    "                outfile.write(line + '\\n')\n",
    "            \n",
    "            # Write Data\n",
    "            for point_line in processed_points:\n",
    "                outfile.write(point_line + '\\n')\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(\"✅ Processing complete!\")\n",
    "        print(f\"  Total points in original file: {len(points)}\")\n",
    "        print(f\"  Points removed (Duplicates): {points_removed_duplicate}\")\n",
    "        print(f\"  Points removed (Classification={classification_to_remove}): {points_removed_classification}\")\n",
    "        print(f\"  Total points in new file: **{final_vertex_count}**\")\n",
    "        print(f\"New file saved as: **{output_filename}**\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the file: {e}\")\n",
    "\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = 'A1-A10_ongoing6.ply' # Change this to your actual file name\n",
    "OUTPUT_FILE = 'A1-A10_ongoing7.ply' \n",
    "CLASSIFICATION_TO_REMOVE = 12.0 # The value you want to filter out\n",
    "\n",
    "# Run the function\n",
    "# IMPORTANT: Ensure 'input.ply' is in the same directory as this script.\n",
    "process_ply_file(INPUT_FILE, OUTPUT_FILE, CLASSIFICATION_TO_REMOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1e9c0-93ce-4b31-ba65-bd17513deabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def filter_and_retype_ply(input_filename, output_filename, value_to_remove=12.0):\n",
    "    \"\"\"\n",
    "    Reads an ASCII PLY file, removes points with a specific \n",
    "    'scalar_Classification' value, and changes the property type \n",
    "    in the header from float to int.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Reading from: {input_filename}\")\n",
    "    print(f\"Writing to: {output_filename}\")\n",
    "    print(f\"Removing points where scalar_Classification is {value_to_remove}...\")\n",
    "    \n",
    "    if not os.path.exists(input_filename):\n",
    "        print(f\"Error: Input file not found at '{input_filename}'\")\n",
    "        return\n",
    "\n",
    "    # Indices assumed from your header structure:\n",
    "    # x: 0, y: 1, z: 2, scalar_Classification: 3, scalar_Original_cloud_index: 4\n",
    "    CLASSIFICATION_INDEX = 3\n",
    "    \n",
    "    header_lines = []\n",
    "    vertex_data_lines = []\n",
    "    in_header = True\n",
    "    original_vertex_count = 0\n",
    "    \n",
    "    try:\n",
    "        # --- 1. Read and Process the Input File ---\n",
    "        with open(input_filename, 'r') as infile:\n",
    "            for line in infile:\n",
    "                line = line.strip()\n",
    "                if not line: # Skip empty lines\n",
    "                    continue\n",
    "                \n",
    "                if in_header:\n",
    "                    header_lines.append(line)\n",
    "                    if line == 'end_header':\n",
    "                        in_header = False\n",
    "                else:\n",
    "                    original_vertex_count += 1\n",
    "                    parts = line.split()\n",
    "                    \n",
    "                    if len(parts) > CLASSIFICATION_INDEX:\n",
    "                        try:\n",
    "                            current_classification = float(parts[CLASSIFICATION_INDEX])\n",
    "                            \n",
    "                            # --- Filter Step: Keep only if classification is NOT the value to remove ---\n",
    "                            if current_classification != value_to_remove:\n",
    "                                # --- Retype Step: Change classification to int format for writing ---\n",
    "                                parts[CLASSIFICATION_INDEX] = str(int(current_classification))\n",
    "                                vertex_data_lines.append(' '.join(parts))\n",
    "                                \n",
    "                        except ValueError:\n",
    "                            # If parsing fails, treat it as non-vertex data and skip filtering/retyping\n",
    "                            vertex_data_lines.append(line)\n",
    "                    else:\n",
    "                        # Keep lines that aren't long enough to be standard vertex data\n",
    "                        vertex_data_lines.append(line)\n",
    "\n",
    "\n",
    "        # --- 2. Modify Header and Prepare Output ---\n",
    "        modified_header = []\n",
    "        new_vertex_count = len(vertex_data_lines)\n",
    "        \n",
    "        for line in header_lines:\n",
    "            if line.startswith('element vertex'):\n",
    "                # Update the vertex count\n",
    "                modified_header.append(f'element vertex {new_vertex_count}')\n",
    "            elif line.startswith('property float scalar_Classification'):\n",
    "                # Change the type from float to int\n",
    "                modified_header.append('property int scalar_Classification')\n",
    "            else:\n",
    "                # Keep other header lines as they are\n",
    "                modified_header.append(line)\n",
    "\n",
    "        # --- 3. Write the Output File ---\n",
    "        with open(output_filename, 'w') as outfile:\n",
    "            # Write the modified header\n",
    "            outfile.write('\\n'.join(modified_header) + '\\n')\n",
    "            \n",
    "            # Write the filtered and retyped vertex data\n",
    "            outfile.write('\\n'.join(vertex_data_lines) + '\\n')\n",
    "\n",
    "        # --- 4. Summary ---\n",
    "        points_removed = original_vertex_count - new_vertex_count\n",
    "        \n",
    "        print(\"-\" * 45)\n",
    "        print(f\"✅ Processing complete!\")\n",
    "        print(f\"Original points: {original_vertex_count}\")\n",
    "        print(f\"Points removed (Classification=12.0): **{points_removed}**\")\n",
    "        print(f\"Final points count: {new_vertex_count}\")\n",
    "        print(f\"New file saved as: **{output_filename}**\")\n",
    "        print(\"Note: scalar_Classification property type is now 'int' in the header.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# IMPORTANT: Change 'input.ply' to the actual name of your file\n",
    "INPUT_FILE = 'A1-A10_ongoing6.ply' \n",
    "OUTPUT_FILE = 'A1-A10_ongoing7.ply' \n",
    "\n",
    "# Run the function\n",
    "filter_and_retype_ply(INPUT_FILE, OUTPUT_FILE, value_to_remove=12.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
